# Copyright 2024 Amazon.com, Inc. or its affiliates.
# All Rights Reserved.

import logging
import pytest
import time

from raff.burst.burst_test import (BurstTest, setup_teardown_burst,
                                   burst_build_to_acquire)
from raff.common.aws_clients.redshift_client import RedshiftClient
from raff.common.base_test import FailedTestException
from raff.common.profile import Profiles
from raff.common.region import DEFAULT_REGION
from raff.common.retry_helper import (log_retry, DEFAULT_RETRY_CONFIG)
from raff.util.utils import run_bootstrap_sql
from retrying import retry

log = logging.getLogger(__name__)

# Trick to get around claim of unused import even though it is used in the
# usefixtures marker.
__all__ = ["setup_teardown_burst", "burst_build_to_acquire"]

tpcds_query = '''
select cc_division_name, cc_call_center_sk, cc_gmt_offset from call_center
limit 2
'''

test_query_setup = '''
create table tbl (a int, b int);
insert into tbl values (1, 2), (3, 4), (5, 6);
'''

test_query = '''
select * from tbl limit 2;
'''

sql_get_last_query_id = '''
select pg_last_query_id()
'''

sql_get_burst_query_id = '''
select concurrency_scaling_query from stl_concurrency_scaling_query_mapping
where primary_query = {}
'''

# When we don't have the session where the previous query is executed,
# we need to use this SQL to get the query id.
sql_get_previous_query_id = '''
select query from stl_query where querytxt like '%{}%' order by starttime desc limit 1;
'''

sql_num_of_segs_get_from_caas = '''
select
  count(distinct path) as cnt
from
  stl_compile_service
where
  lookupsuccess = 1 and
  query={0}
'''

sql_num_of_segs_put_into_caas = '''
select
  count(distinct path) as cnt
from
  stl_compile_service
where
  putsuccess = 1 and
  (query ={0} or
  query = (select codegenquery from stl_codegen where query={0}))
'''

sql_is_burst_cluster_personalized = '''
select count(*) from stl_burst_manager_personalization where trim(cluster_arn)='{}'
'''

sql_num_of_unique_segs_given_predicate = '''
WITH cinfo AS (
  SELECT
    PATH
  FROM
    stl_compile_info
  WHERE
    query = {{0}}
    and ({0})
union all
  SELECT
    PATH
  FROM
    stl_bkg_compile_info
  WHERE
    query = {{0}}
    and ({0})
)
SELECT
  count(distinct path)
FROM
  cinfo
'''

# while compiling, we might get global cache hits as we put code
# segments to CaaS before the compilation. That's why we add
# cachehit 6 (GlobalSharedDDBCacheTryCompile) and 8
# (RegionSharedS3CacheTryCompile) here.
sql_num_of_compiled_segs = sql_num_of_unique_segs_given_predicate.format(
    "cachehit = 0 or cachehit = 6 or cachehit = 8")

# Here cachehit 4 and 7 corresponds to GlobalSharedDDBCache and
# RegionSharedS3Cache.
sql_num_of_global_cache_hits = sql_num_of_unique_segs_given_predicate.format(
    "cachehit = 4 or cachehit = 7")

sql_segment_details = '''
select * from (
(select
   'stl_compile_info' as table_name, query, segment, cachehit, path::text
from
  stl_compile_info
where
  query = {0})
union all
(select
   'stl_bkg_compile_info' as table_name, query, segment, cachehit, path::text
from
  stl_bkg_compile_info
where
  query = {0})
) order by table_name, segment;
'''

@pytest.mark.serial_only
@pytest.mark.load_tpcds_data
@pytest.mark.cluster_only
# Skip these codegen-specific tests in Precompiled Executors configurations.
@pytest.mark.cg_only
@pytest.mark.usefixtures("setup_teardown_burst")
@pytest.mark.custom_burst_gucs(
    gucs={
        'enable_burst_async_acquire':
        'false',
        # With QLCG, the number of cold compiled segments is
        # unpredictable. So, we are disabling it.
        'burst_propagated_static_gucs':
        '{"padbGucs": [{ '
        '"name": "enable_query_level_code_generation", '
        '"value": "false"}]}',
    })
class TestBurstWithCaaS(BurstTest):
    """
    Test that burst cluster uses CaaS to reuse code segments generated by the
    same or another burst cluster.
    """

    def test_caas_works_with_burst_before_personalization(
            self, db_session, cluster, burst_build_to_acquire):
        """
        This test verifies following scenarios before the burst cluster is
        personalized.
        1. Compiled segments on burst clusters are successfully put to S3
           to be consumed by CaaS.
        2. A Burst cluster downloads compiled objects from the CaaS which were
           uploaded to the CaaS by the same cluster.
        3. A new burst cluster downloads compiled objects from the CaaS
           when a query runs for the first time in the cluster, but the same
           query has been executed in different cluster previously.
        Args:
            db_session: Redshift DB session for main cluster.
            cluster: Redshift cluster instance
            : acquire burst cluster without
                checking that PADB version on burst matches main cluster.
            burst_build_to_acquire: burst cluster build number to acquire
        """
        test_marker = int(time.time())
        log.info("Test time marker: {}".format(int(test_marker)))
        self._acquire_burst_cluster(cluster, burst_build_to_acquire)
        burst_cluster = self._get_burst_cluster(cluster)
        run_bootstrap_sql(burst_cluster, test_query_setup)
        burst_query_id = self._run_query_directly_in_burst_cluster(
            test_marker, burst_cluster)
        (num_of_compiled_segs,
         seg_details_in_cold_run) = self._verify_cold_run_stats(
             burst_cluster, burst_query_id)
        # On the same cluster, test running the same query on cold-local
        # and warm-global cache.
        self._run_multiple_with_global_warm_cache(test_marker, burst_cluster,
                                                  num_of_compiled_segs,
                                                  seg_details_in_cold_run)
        assert self._is_burst_cluster_personalized(cluster) is False, (
            "Burst cluster is personalized.")
        # Release all the burst clusters and acquire a new burst cluster.
        # Test the same query run on cold-local and warm-global cache on
        # the new burst cluster.
        self._acquire_burst_cluster(cluster, burst_build_to_acquire)
        burst_cluster = self._get_burst_cluster(cluster)
        run_bootstrap_sql(burst_cluster, test_query_setup)
        self._run_multiple_with_global_warm_cache(test_marker, burst_cluster,
                                                  num_of_compiled_segs,
                                                  seg_details_in_cold_run)
        assert self._is_burst_cluster_personalized(cluster) is False, (
            "Burst cluster is personalized.")

    def test_caas_works_with_burst_after_personalization(
            self, db_session, cluster, burst_build_to_acquire):
        """
        This test verifies following scenarios after the burst cluster is
        personalized.
        1. Compiled segments on burst clusters are successfully put to S3
           to be consumed by CaaS.
        2. A Burst cluster downloads compiled objects from the CaaS which were
           uploaded to the CaaS by the same cluster.
        3. A new burst cluster downloads compiled objects from the CaaS
           when a query runs for the first time in the cluster, but the same
           query has been executed in different cluster previously.
        Args:
            db_session: Redshift DB session for main cluster.
            cluster: Redshift cluster instance
            : acquire burst cluster without
                checking that PADB version on burst matches main cluster.
            burst_build_to_acquire: burst cluster build number to acquire
        """
        test_marker = int(time.time())
        log.info("Test time marker: {}".format(int(test_marker)))
        with self.burst_db_cursor(db_session) as cursor:
            self._acquire_and_personalize_burst_cluster(
                cluster, cursor, burst_build_to_acquire)
            burst_cluster = self._get_burst_cluster(cluster)
            burst_query_id = self._run_burstable_query_in_main_cluster(
                test_marker, cursor, burst_cluster)
            (num_of_compiled_segs,
             seg_details_in_cold_run) = self._verify_cold_run_stats(
                 burst_cluster, burst_query_id)
            # On the same cluster, test running the same query on cold-local
            # and warm-global cache.
            self._run_multiple_with_global_warm_cache(
                test_marker, burst_cluster, num_of_compiled_segs,
                seg_details_in_cold_run, cursor)
            # Release all the burst clusters and acquire a new burst cluster.
            # Test the same query run on cold-local and warm-global cache on
            # the new burst cluster.
            self._acquire_and_personalize_burst_cluster(
                cluster, cursor, burst_build_to_acquire)
            burst_cluster = self._get_burst_cluster(cluster)
            self._run_multiple_with_global_warm_cache(
                test_marker, burst_cluster, num_of_compiled_segs,
                seg_details_in_cold_run, cursor)

    def _acquire_and_personalize_burst_cluster(self, cluster, cursor,
                                               burst_build_to_acquire):
        """
        This will release any previously attached burst clusters and create
        and personalize a new burst cluster.

        Args:
            cluster: Main cluster instance
            cursor: Regular session cursor on main cluster
            : acquire burst cluster without
                checking that PADB version on burst matches main cluster.
            burst_build_to_acquire: burst cluster build number to acquire
        """
        self._acquire_burst_cluster(cluster, burst_build_to_acquire)
        # Burst cluster may take a few seconds to cold start. Running this
        # function can ensure the cold start and personalization process is
        # done.
        cursor.execute(tpcds_query)
        _ = cursor.fetchall()
        assert self._is_burst_cluster_personalized(cluster), (
            "Burst cluster is not personalized.")

    def _acquire_burst_cluster(self, cluster, burst_build_to_acquire):
        """
        This will release any previously attached burst clusters and acquire a
        new burst cluster.

        Args:
            cluster: Main cluster instance
            : acquire burst cluster without
                checking that PADB version on burst matches main cluster.
            burst_build_to_acquire: burst cluster build number to acquire
        """
        log.info("Release any previously attached burst clusters.")
        cluster.release_all_burst_clusters()
        log.info("Acquire a burst cluster.")
        cluster.acquire_burst_cluster(burst_build_to_acquire)

    def _get_burst_cluster(self, cluster):
        """
        Get the burst cluster instance.

        Args:
            cluster: Main cluster instance

        Returns: Burst cluster instance if one is attached.
        """
        burst_cluster_arn = self._get_burst_cluster_arn(cluster)
        burst_cluster_id = burst_cluster_arn.split(':cluster:')[-1].strip()
        log.info("Burst cluster id: {}".format(burst_cluster_id))
        client = RedshiftClient(
            profile=Profiles.QA_BURST_TEST, region=DEFAULT_REGION)
        return client.describe_cluster(burst_cluster_id)

    def _verify_cold_run_stats(self, burst_cluster, burst_query_id):
        """
        Asserts that when the local cache and global cache is cold, all segments
        will be compiled and uploaded to the CaaS.

        Args:
            burst_cluster: Burst cluster instance
            burst_query_id: Query id of the interested query executed in the
                            burst cluster
        """
        num_of_compiled_segs = run_bootstrap_sql(
            burst_cluster,
            sql_num_of_compiled_segs.format(burst_query_id))[0][0]
        log.info("Number of compiled segments in the cold run: {}".format(
            num_of_compiled_segs))
        # For troubleshooting purpose, get the segment details
        seg_details_in_cold_run = run_bootstrap_sql(
            burst_cluster, sql_segment_details.format(burst_query_id))
        log.info("Segment details in the cold run: "
                 "{}".format(seg_details_in_cold_run))
        num_of_segs_put_into_caas = run_bootstrap_sql(
            burst_cluster,
            sql_num_of_segs_put_into_caas.format(burst_query_id))[0][0]
        log.info(
            "Number of segments put into the CaaS in the cold run: {}".format(
                num_of_segs_put_into_caas))
        assert int(num_of_compiled_segs) > 0, (
            "In the cold run, number "
            "of compiled segments should be a positive number.")
        message = (
            "In the cold run, number of compiled segments should match "
            "with the number of segments put into the CaaS. Compiled: {} "
            "Put: {}".format(num_of_compiled_segs, num_of_segs_put_into_caas))
        assert int(num_of_compiled_segs) == int(
            num_of_segs_put_into_caas), message
        num_of_segs_get_from_caas = run_bootstrap_sql(
            burst_cluster,
            sql_num_of_segs_get_from_caas.format(burst_query_id))[0][0]
        log.info("Number of segments get from the CaaS in the cold run: {}"
                 .format(num_of_segs_get_from_caas))
        message = ("Expected no global cache hits in the cold run. "
                   "Compiled: {} Put: {} Get:{}".format(
                       num_of_compiled_segs, num_of_segs_put_into_caas,
                       num_of_segs_get_from_caas))
        assert int(num_of_segs_get_from_caas) == int(0), message
        return (num_of_compiled_segs, seg_details_in_cold_run)

    def _verify_warm_run_stats(
            self, test_marker, burst_cluster, burst_query_id,
            num_of_compiled_segs_in_cold_run, seg_details_in_cold_run):
        """
        Asserts that when the global cache is warmed up, there are no
        compilations and we got as many global cache hits as the compilations
        in the cold run.

        Args:
            test_marker: Marker injected into the code generation in the cold
                         run.
            burst_cluster: Burst cluster instance
            burst_query_id: Query id of the interested query executed in the
                            burst cluster
            num_of_compiled_segs_in_cold_run: Number of the compiled segments
                                              in the cold run
            seg_details_in_cold_run: Segment details in the cold run which will
                                     be logged for troubleshooting purposes.
        """
        num_of_global_cache_hits = run_bootstrap_sql(
            burst_cluster,
            sql_num_of_global_cache_hits.format(burst_query_id))[0][0]
        log.info("Number of global cache hits in the warm run: {}"
                 .format(num_of_global_cache_hits))
        message = (
            "Expected global cache hits for all the segments in the warm run."
            " Global cache hits: {} Cold Compiled: {}".format(
                num_of_global_cache_hits, num_of_compiled_segs_in_cold_run))
        # For troubleshooting purpose, get the segment details
        seg_details_in_warm_run = run_bootstrap_sql(
            burst_cluster, sql_segment_details.format(burst_query_id))
        log.info("Warm run segment paths: "
                 "{}".format(seg_details_in_warm_run))
        message += (" Cold run: {}".format(seg_details_in_cold_run))
        message += (" Warm run: {}".format(seg_details_in_warm_run))

        if int(num_of_global_cache_hits) != int(
                num_of_compiled_segs_in_cold_run):
            raise FailedTestException(message)

        num_of_segs_put_into_caas = run_bootstrap_sql(
            burst_cluster,
            sql_num_of_segs_put_into_caas.format(burst_query_id))[0][0]
        log.info("Number of segments put into the CaaS in the warm run: {}"
                 .format(num_of_segs_put_into_caas))
        message = ("In the warm run, no segments should be put into CaaS"
                   " Put: {}".format(num_of_segs_put_into_caas))
        message += (" Cold run: {}".format(seg_details_in_cold_run))
        message += (" Warm run: {}".format(seg_details_in_warm_run))

        if int(num_of_segs_put_into_caas) != int(0):
            raise FailedTestException(message)

    def _run_burstable_query_in_main_cluster(self, test_marker, cursor,
                                             burst_cluster):
        # Clear the local cache
        burst_cluster.run_xpx('code_pool purge_local_cache')
        # Inject the same maker to the code generation which was injected
        # during the cold run.
        burst_cluster.set_event('EtCodegenInjectMarker, marker={}'.format(
            int(test_marker)))
        # Issue the query on the main cluster, this will be run on
        # burst cluster.
        cursor.execute(tpcds_query)
        burst_cluster.unset_event('EtCodegenInjectMarker')
        cursor.execute(sql_get_last_query_id)
        query_id = cursor.fetch_scalar()
        log.info("Query ID on main cluster: {}".format(query_id))
        burst_query_id = run_bootstrap_sql(
            burst_cluster, sql_get_burst_query_id.format(query_id))[0][0]
        log.info("Query ID on burst cluster: {}".format(burst_query_id))
        return burst_query_id

    def _run_query_directly_in_burst_cluster(self, test_marker, burst_cluster):
        # Clear the local cache
        burst_cluster.run_xpx('code_pool purge_local_cache')
        # Inject the same maker to the code generation which was injected
        # during the cold run.
        burst_cluster.set_event('EtCodegenInjectMarker, marker={}'.format(
            int(test_marker)))
        # Because we need to run this query before personalization,
        # we need to issue query in the burst cluster. If we run
        # a burstable query in the main cluster, it will personalize
        # burst cluster automatically.
        run_bootstrap_sql(burst_cluster, test_query)
        burst_cluster.unset_event('EtCodegenInjectMarker')
        burst_query_id = run_bootstrap_sql(burst_cluster,
                                           sql_get_previous_query_id.format(
                                               test_query.strip()))[0][0]
        log.info("Query ID on burst cluster: {}".format(burst_query_id))
        return burst_query_id

    def _is_burst_cluster_personalized(self, cluster):
        burst_cluster_arn = self._get_burst_cluster_arn(cluster)
        is_burst_cluster_personalized = run_bootstrap_sql(
            cluster,
            sql_is_burst_cluster_personalized.format(burst_cluster_arn))[0][0]
        return int(is_burst_cluster_personalized) > 0

    def _get_burst_cluster_arn(self, cluster):
        results = cluster.list_acquired_burst_clusters()
        assert len(results) > 0, ("Cannot get burst cluster arn.")
        return results[0].strip()

    @retry(
        stop_max_attempt_number=5,
        wait_func=log_retry(DEFAULT_RETRY_CONFIG.exponential_sleep))
    def _run_multiple_with_global_warm_cache(self,
                                             test_marker,
                                             burst_cluster,
                                             num_of_compiled_segs_in_cold_run,
                                             seg_details_in_cold_run,
                                             cursor=None):
        """
        Asserts that when the global cache is warmed up, there are no
        compilations and we got as many global cache hits as the compilations
        in the cold run. We run the test multiple times because the compiled
        objects in CaaS might not ready although we uploaded the cpp file to
        the CaaS during the cold run.

        Args:
            test_marker: Marker injected into the code generation in the cold
                         run.
            burst_cluster: Burst cluster instance
            num_of_compiled_segs_in_cold_run: Number of the compiled segments
                                              in the cold run
            seg_details_in_cold_run: Segment details in the cold run which will
                                     be logged for troubleshooting purposes.
            cursor: If the query needs to be run in the main cluster, we should
                    pass session cursor on main cluster, If the query needs to
                    be run in the burst cluster directly, we should pass None here.
        """
        if cursor is not None:
            burst_query_id = self._run_burstable_query_in_main_cluster(
                test_marker, cursor, burst_cluster)
        else:
            burst_query_id = self._run_query_directly_in_burst_cluster(
                test_marker, burst_cluster)
        self._verify_warm_run_stats(test_marker, burst_cluster, burst_query_id,
                                    num_of_compiled_segs_in_cold_run,
                                    seg_details_in_cold_run)
